{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:01:16.289883Z",
     "start_time": "2020-03-24T11:01:16.284339Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:01:22.252547Z",
     "start_time": "2020-03-24T11:01:17.559201Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, io\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "from cleverhans import utils_tf\n",
    "from util import cvt2Image, sparse_tuple_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:01:38.827402Z",
     "start_time": "2020-03-24T11:01:22.257045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint version 1 is up-to-date.\n",
      "charset: ['', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}']\n",
      "Using CUDNN LSTM backend on GPU\n",
      "Successfully load the model's weights\n",
      "INFO:tensorflow:Restoring parameters from /home/chenlu/calamari/models/antiqua_modern/4.ckpt\n"
     ]
    }
   ],
   "source": [
    "from calamari_ocr.ocr.backends.tensorflow_backend.tensorflow_model import TensorflowModel\n",
    "from calamari_ocr.ocr import Predictor\n",
    "checkpoint = '/home/chenlu/calamari/models/antiqua_modern/4.ckpt.json'\n",
    "predictor = Predictor(checkpoint=checkpoint, batch_size=1, processes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:01:38.841058Z",
     "start_time": "2020-03-24T11:01:38.832579Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = predictor.network\n",
    "sess, graph = network.session, network.graph\n",
    "codec = network.codec\n",
    "charset = codec.charset\n",
    "encode, decode = codec.encode, codec.decode\n",
    "code2char, char2code = codec.code2char, codec.char2code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:01:38.855981Z",
     "start_time": "2020-03-24T11:01:38.845229Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def invert(data): # 反色\n",
    "    if data.max() < 1.5:\n",
    "        return 1 - data\n",
    "    else:\n",
    "        return 255 - data\n",
    "\n",
    "def transpose(data): # 旋转90度\n",
    "    if len(data.shape) != 2:\n",
    "        return np.swapaxes(data, 1, 2)\n",
    "    else:\n",
    "        return data.T\n",
    "\n",
    "def cvt2raw(data):\n",
    "    return transpose(invert(data))\n",
    "\n",
    "def show(img):\n",
    "    return cvt2Image(cvt2raw(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T16:23:10.894149Z",
     "start_time": "2020-03-31T16:23:08.374414Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_data_path = '/home/chenlu/research/TextRecognitionDataGenerator/word_image_data'\n",
    "font_name, case, pert_type, eps, eps_iter, nb_iter = 'Arial', 'easy', '2', 0.2, 5.0,1000 \n",
    "\n",
    "with open(f'{img_data_path}/{font_name}-{case}.pkl', 'rb') as f:\n",
    "    _, input_img, len_x, gt_txt, target_txt = pickle.load(f)\n",
    "input_img = np.asarray(input_img)\n",
    "\n",
    "title = f\"{font_name}-{case}-l{pert_type}-eps{eps}-ieps{eps_iter}-iter{nb_iter}\"\n",
    "with open(f'wm_result/{title}.pkl', 'rb') as f:\n",
    "    (_, wm0_mask, _, wm0_img, record_text, _, adv_img, record_adv_text, record_iter, _, _) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T16:23:11.062903Z",
     "start_time": "2020-03-31T16:23:10.899351Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_data_path = '/home/chenlu/research/TextRecognitionDataGenerator/word_image_data'\n",
    "with open(f'{img_data_path}/intersect_word.pkl', 'rb') as f:\n",
    "    intersect_word = pickle.load(f)\n",
    "intersect_word = set(intersect_word)\n",
    "\n",
    "def filter_word(input_img, len_x, gt_txt, target_txt):\n",
    "    new_input_img, new_len_x, new_gt_txt, new_target_txt = [], [], [], []\n",
    "    for i, gt in enumerate(gt_txt):\n",
    "        if gt in intersect_word:\n",
    "            new_input_img.append(input_img[i])\n",
    "            new_len_x.append(len_x[i])\n",
    "            new_gt_txt.append(gt_txt[i])\n",
    "            new_target_txt.append(target_txt[i])\n",
    "    return np.asarray(new_input_img), new_len_x, new_gt_txt, new_target_txt\n",
    "input_img, len_x, gt_txt, target_txt = filter_word(input_img, len_x, gt_txt, target_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T16:24:01.029732Z",
     "start_time": "2020-03-31T16:24:00.983676Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_sample = 1000\n",
    "len_x, target_txt = len_x[:nb_sample], target_txt[:nb_sample]\n",
    "input_img = input_img[:nb_sample]\n",
    "\n",
    "wm0_img = wm0_img[:nb_sample]\n",
    "wm_mask_img = wm0_mask[:nb_sample].astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T16:19:42.683347Z",
     "start_time": "2020-03-31T16:19:42.676683Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = tuple([batch_size] + list(input_img.shape[1:]))  # (batch_size, height, width, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T16:20:04.497581Z",
     "start_time": "2020-03-31T16:20:04.487106Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = learning_rate = 0.01 # float\n",
    "ABORT_EARLY = abort_early = True # bool\n",
    "batch_size = batch_size = 100 # int\n",
    "clip_min, clip_max = 0.0, 1.0\n",
    "np_dtype = np.dtype('float32')\n",
    "tf_dtype = tf.as_dtype('float32')\n",
    "\n",
    "MAX_ITERATIONS = 1000\n",
    "BINARY_SEARCH_STEPS = 1\n",
    "initial_const = 100  # float\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T03:41:21.279856Z",
     "start_time": "2020-04-10T03:41:20.518734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDNN LSTM backend on GPU\n"
     ]
    }
   ],
   "source": [
    "# build graph\n",
    "with graph.as_default():\n",
    "    # the variable we're going to optimize over\n",
    "    modifier = tf.Variable(np.zeros(shape, dtype=np_dtype))  # (batch_size, height, width, channel)\n",
    "    # these are variables to be more efficient in sending data to tf\n",
    "    timg = tf.Variable(np.zeros(shape), dtype=tf_dtype, name='timg')\n",
    "    input_seq_len = tf.Variable(np.zeros(batch_size), dtype=tf.int32, name='seq_len')\n",
    "    const = tf.Variable(np.zeros(batch_size), dtype=tf_dtype, name='const')\n",
    "    # and here's what we use to assign them\n",
    "    assign_timg, assign_input_seq_len, targets, dropout_rate, _, _ = network.create_placeholders()\n",
    "    assign_const = tf.placeholder(tf_dtype, [batch_size], name='assign_const')\n",
    "    \n",
    "    # the resulting instance, tanh'd to keep bounded from clip_min to clip_max\n",
    "    newimg = (tf.tanh(modifier + timg) + 1) / 2\n",
    "    newimg = newimg * (clip_max - clip_min) + clip_min \n",
    "    # prediction BEFORE-SOFTMAX of the model\n",
    "    output_seq_len, time_major_logits, time_major_softmax, logits, softmax, decoded, sparse_decoded, scale_factor, \\\n",
    "            log_prob = network.create_network(newimg, input_seq_len, dropout_rate, reuse_variables=tf.AUTO_REUSE)\n",
    "    # distance to the input data\n",
    "        \n",
    "    # ctc loss\n",
    "    loss1 = tf.nn.ctc_loss(labels=targets,\n",
    "                           inputs=time_major_logits,\n",
    "                           sequence_length=output_seq_len,\n",
    "                           time_major=True,\n",
    "                           ctc_merge_repeated=True,\n",
    "                           ignore_longer_outputs_than_inputs=True)\n",
    "    loss1 = tf.reduce_sum(const * loss1) # mu * ctc_loss\n",
    "    \n",
    "    # L2-norm distance\n",
    "    other = (tf.tanh(timg) + 1) / 2 * (clip_max - clip_min) + clip_min  \n",
    "    l2dist = tf.reduce_sum(tf.square(newimg - other), axis=list(range(1, len(shape))))\n",
    "    loss2 = tf.reduce_sum(l2dist) \n",
    "    \n",
    "    # nps: non-printability score\n",
    "    closest_val = tf.round(newimg * 255)  # 乘 255 取整\n",
    "    loss3 = 1e-2 * tf.reduce_sum(tf.abs(255 * newimg - closest_val))\n",
    "\n",
    "    # tv: total variation\n",
    "    n_x, n_y = input_img.shape[1], input_img.shape[2]\n",
    "    tmp1 = modifier[:, :n_x - 1, :n_y - 1]\n",
    "    tmp2 = modifier[:, 1:n_x, :n_y - 1]\n",
    "    tmp3 = modifier[:, :n_x - 1, 1:n_y]\n",
    "    tv = tf.sqrt(tf.square(tmp1 - tmp2) + tf.square(tmp1 - tmp3) + 1e-5)\n",
    "    loss4 = tf.reduce_sum(1e-3 * tv)\n",
    "    \n",
    "    loss = loss1 + loss2 + loss4\n",
    "    \n",
    "    # Setup the adam optimizer and keep track of variables we're creating\n",
    "    start_vars = set(x.name for x in tf.global_variables())\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "    train = optimizer.minimize(loss, var_list=[modifier])\n",
    "    end_vars = tf.global_variables()\n",
    "    new_vars = [x for x in end_vars if x.name not in start_vars] # 优化在 optimizer 和 train 里的变量\n",
    "    \n",
    "    # these are the variables to initialize when we run\n",
    "    setup = []\n",
    "    setup.append(timg.assign(assign_timg))\n",
    "    setup.append(input_seq_len.assign(assign_input_seq_len))\n",
    "    setup.append(const.assign(assign_const))\n",
    "\n",
    "    init = tf.variables_initializer(var_list=[modifier] + new_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## build graph + eps constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T03:39:25.514687Z",
     "start_time": "2020-02-27T03:39:24.842497Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# build graph\n",
    "with graph.as_default():\n",
    "    # the variable we're going to optimize over\n",
    "    modifier = tf.Variable(np.zeros(shape, dtype=np_dtype))  # (batch_size, height, width, channel)\n",
    "    # these are variables to be more efficient in sending data to tf\n",
    "    timg = tf.Variable(np.zeros(shape), dtype=tf_dtype, name='timg')\n",
    "    input_seq_len = tf.Variable(np.zeros(batch_size), dtype=tf.int32, name='seq_len')\n",
    "    const = tf.Variable(np.zeros(batch_size), dtype=tf_dtype, name='const')\n",
    "    # and here's what we use to assign them\n",
    "    assign_timg, assign_input_seq_len, targets, dropout_rate, _, _ = network.create_placeholders()\n",
    "    assign_const = tf.placeholder(tf_dtype, [batch_size], name='assign_const')\n",
    "    \n",
    "    # the resulting instance, tanh'd to keep bounded from clip_min to clip_max\n",
    "    newimg = (tf.tanh(modifier + timg) + 1) / 2\n",
    "    newimg = newimg * (clip_max - clip_min) + clip_min \n",
    "#     newimg = tf.clip_by_value(0.2 * tf.tanh(modifier) + timg, clip_min, clip_max)\n",
    "#     newimg = (tf.tanh(0.5 * tf.tanh(modifier) + timg) + 1) / 2\n",
    "\n",
    "    # prediction BEFORE-SOFTMAX of the model\n",
    "    output_seq_len, time_major_logits, time_major_softmax, logits, softmax, decoded, sparse_decoded, scale_factor, \\\n",
    "            log_prob = network.create_network(newimg, input_seq_len, dropout_rate, reuse_variables=tf.AUTO_REUSE)\n",
    "    # distance to the input data\n",
    "    other = (tf.tanh(timg) + 1) / 2 * (clip_max - clip_min) + clip_min  \n",
    "    l2dist = tf.reduce_sum(tf.square(newimg - other), axis=list(range(1, len(shape))))  # (batch_size, )\n",
    "       \n",
    "    # sum up the losses\n",
    "    loss1 = tf.nn.ctc_loss(labels=targets,\n",
    "                           inputs=time_major_logits,\n",
    "                           sequence_length=output_seq_len,\n",
    "                           time_major=True,\n",
    "                           ctc_merge_repeated=True,\n",
    "                           ignore_longer_outputs_than_inputs=True)\n",
    "    loss1 = tf.reduce_sum(const * loss1) # mu * ctc_loss\n",
    "    loss2 = tf.reduce_sum(l2dist) # L2-norm distance\n",
    "    loss = loss1 + loss2\n",
    "    \n",
    "    # Setup the adam optimizer and keep track of variables we're creating\n",
    "    start_vars = set(x.name for x in tf.global_variables())\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "    train = optimizer.minimize(loss, var_list=[modifier])\n",
    "    end_vars = tf.global_variables()\n",
    "    new_vars = [x for x in end_vars if x.name not in start_vars] # 优化在 optimizer 和 train 里的变量\n",
    "    \n",
    "    # these are the variables to initialize when we run\n",
    "    setup = []\n",
    "    setup.append(timg.assign(assign_timg))\n",
    "    setup.append(input_seq_len.assign(assign_input_seq_len))\n",
    "    setup.append(const.assign(assign_const))\n",
    "\n",
    "    init = tf.variables_initializer(var_list=[modifier] + new_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T03:41:21.862258Z",
     "start_time": "2020-04-10T03:41:21.798601Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess img\n",
    "# imgs = input_img[:len(input_img) // batch_size * batch_size]\n",
    "imgs = input_img[:batch_size]\n",
    "imgs = (imgs - clip_min) / (clip_max - clip_min) # re-scale instances to be within range [0, 1]\n",
    "imgs = np.clip(imgs, clip_min, clip_max)\n",
    "imgs = (imgs * 2) - 1 # now convert to [-1, 1]\n",
    "imgs = np.arctanh(imgs * .999999) # convert to tanh-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T03:42:07.430754Z",
     "start_time": "2020-04-10T03:41:22.222040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Binary search step 0 of 1\n",
      "Iteration 0 of 1000 loss=316270.0 ctc_loss=316267.1 l2s=0.000 tv=2.854\n",
      "acc: 0.0\n",
      "Iteration 100 of 1000 loss=39492.6 ctc_loss=37707.1 l2s=16.846 tv=100.849\n",
      "acc: 0.47\n",
      "Iteration 200 of 1000 loss=5321.8 ctc_loss=3187.0 l2s=20.284 tv=106.456\n",
      "acc: 0.95\n",
      "Iteration 300 of 1000 loss=3076.4 ctc_loss=1182.5 l2s=17.968 tv=97.049\n",
      "acc: 0.98\n",
      "Iteration 400 of 1000 loss=2355.1 ctc_loss=659.4 l2s=16.061 tv=89.617\n",
      "acc: 0.98\n",
      "Iteration 500 of 1000 loss=1937.2 ctc_loss=399.8 l2s=14.537 tv=83.658\n",
      "acc: 0.99\n",
      "Iteration 600 of 1000 loss=1755.4 ctc_loss=345.2 l2s=13.313 tv=78.895\n",
      "acc: 0.99\n",
      "Iteration 700 of 1000 loss=1615.7 ctc_loss=303.1 l2s=12.374 tv=75.117\n",
      "acc: 0.99\n",
      "Iteration 800 of 1000 loss=1497.5 ctc_loss=264.5 l2s=11.609 tv=72.026\n",
      "acc: 0.99\n",
      "Iteration 900 of 1000 loss=1411.4 ctc_loss=245.9 l2s=10.961 tv=69.375\n",
      "acc: 0.99\n",
      "Successfully generated adversarial examples on 99 of 100 instances.\n",
      "Mean successful distortion: 3.08\n",
      "------------------------------ 0 ------------------------------\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    adv_img_list = []\n",
    "    adv_l2_list = []\n",
    "    adv_txt_list = []\n",
    "    adv_iter_list = []\n",
    "    for i in range(0, len(imgs), batch_size):  # run attack in batch data\n",
    "        # set the lower and upper bounds accordingly\n",
    "        lower_bound = np.zeros(batch_size)  # (batch_size, )\n",
    "        CONST = np.ones(batch_size) * initial_const  # (batch_size, )\n",
    "        upper_bound = np.ones(batch_size) * 1e10  # (batch_size, )\n",
    "\n",
    "        # placeholders for the best l2, score, and instance attack found so far\n",
    "        o_bestl2 = [1e10] * batch_size  # (batch_size, )\n",
    "        o_bestscore = [-1] * batch_size  # (batch_size, )\n",
    "        o_bestattack = np.zeros(shape)\n",
    "        o_bestiter = [-1] * batch_size\n",
    "\n",
    "        for outer_step in range(BINARY_SEARCH_STEPS):  # 二分调整 const\n",
    "            # completely reset adam's internal state.\n",
    "            sess.run(init)\n",
    "            batch = imgs[i:i + batch_size]\n",
    "            batch_len_x = len_x[i:i + batch_size]\n",
    "            batch_target_txt = target_txt[i:i + batch_size]\n",
    "            batch_tmp_y = [\n",
    "                np.asarray([c - 1 for c in encode(t)])\n",
    "                for t in batch_target_txt\n",
    "            ]\n",
    "            batch_y = sparse_tuple_from(batch_tmp_y)\n",
    "\n",
    "            bestl2 = [1e10] * batch_size  # (batch_size, )\n",
    "            bestscore = [-1] * batch_size  # (batch_size, )\n",
    "            bestiter = [-1] * batch_size\n",
    "            print(\n",
    "                f\"  Binary search step {outer_step} of {BINARY_SEARCH_STEPS}\")\n",
    "\n",
    "            # set the variables so that we don't have to send them over again\n",
    "            sess.run(\n",
    "                setup, {\n",
    "                    assign_timg: batch,\n",
    "                    assign_input_seq_len: batch_len_x,\n",
    "                    assign_const: CONST,\n",
    "                })\n",
    "\n",
    "            for iteration in range(MAX_ITERATIONS):  # 开始迭代攻击\n",
    "                # perform the attack\n",
    "                _, l, scores, l2s, batch_adv_txt, nimg, l4 = sess.run(\n",
    "                    [train, loss, loss1, l2dist, decoded, newimg, loss4],\n",
    "                    feed_dict={\n",
    "                        targets: batch_y,\n",
    "                        dropout_rate: 0\n",
    "                    })\n",
    "                batch_adv_index = TensorflowModel._TensorflowModel__sparse_to_lists(batch_adv_txt)\n",
    "                batch_adv_txt = [''.join(decode(index)) for index in batch_adv_index]\n",
    "                # attack done\n",
    "                if iteration % ((MAX_ITERATIONS // 10) or 1) == 0:\n",
    "                    print(f\"Iteration {iteration} of {MAX_ITERATIONS} \" +\n",
    "                          f\"loss={l:.1f} ctc_loss={np.mean(scores):.1f} \" + \n",
    "                          f\"l2s={np.mean(l2s):.3f} tv={np.mean(l4):.3f}\")\n",
    "\n",
    "                # adjust the best result found so far\n",
    "                for e, (l2, ii) in enumerate(zip(l2s, nimg)):\n",
    "                    target_t, adv_t = batch_target_txt[e], batch_adv_txt[e]\n",
    "                    if l2 < bestl2[e] and target_t == adv_t:\n",
    "                        bestl2[e] = l2\n",
    "                        bestscore[e] = adv_t\n",
    "                        if bestiter[e] == -1: # record success iter\n",
    "                            bestiter[e] = iteration\n",
    "                    if l2 < o_bestl2[e] and target_t == adv_t:\n",
    "                        o_bestl2[e] = l2\n",
    "                        o_bestscore[e] = adv_t  # batch_adv_txt\n",
    "                        o_bestattack[e] = ii\n",
    "                        if o_bestiter[e] == -1: # record success iter\n",
    "                            o_bestiter[e] = iteration\n",
    "\n",
    "                # check if we should abort search if we're getting nowhere.\n",
    "                if ABORT_EARLY:\n",
    "                    n_success = 0\n",
    "                    for target_t, adv_t in zip(batch_target_txt, bestscore):\n",
    "                        if target_t == adv_t:\n",
    "                            n_success += 1\n",
    "                    if iteration % ((MAX_ITERATIONS // 10) or 1) == 0:\n",
    "                        print(f'acc: {n_success / len(batch_target_txt)}')\n",
    "                    if n_success == len(batch_target_txt):\n",
    "                        print(f\"iteration[{iteration}] break, all attacks succeed!\")\n",
    "                        break\n",
    "\n",
    "            # adjust the constant as needed\n",
    "            for e in range(batch_size):  # 二分调整 const\n",
    "                if bestscore[e] == batch_target_txt[e]:\n",
    "                    # success, divide const by two\n",
    "                    upper_bound[e] = min(upper_bound[e], CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e]) / 2\n",
    "                else:\n",
    "                    # failure, either multiply by 10 if no solution found yet\n",
    "                    # or do binary search with the known upper bound\n",
    "                    lower_bound[e] = max(lower_bound[e], CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e]) / 2\n",
    "                    else:\n",
    "                        CONST[e] *= 10\n",
    "\n",
    "            print(\n",
    "                f\"Successfully generated adversarial examples on {sum(upper_bound < 1e9)} of {batch_size} instances.\"\n",
    "            )\n",
    "            o_bestl2 = np.array(o_bestl2)\n",
    "            mean = np.mean(np.sqrt(o_bestl2[o_bestl2 < 1e9]))\n",
    "            print(f\"Mean successful distortion: {mean:.4g}\")\n",
    "\n",
    "        # return the best solution found\n",
    "        o_bestl2 = np.array(o_bestl2)\n",
    "        adv_l2_list.append(o_bestl2)\n",
    "        adv_txt_list += o_bestscore\n",
    "        adv_img_list.append(o_bestattack)  # 把 batch adv img 加到 list 中\n",
    "        adv_iter_list += o_bestiter\n",
    "        print('-' * 30, i, '-' * 30)\n",
    "\n",
    "adv_img = np.asarray(adv_img_list).reshape(imgs.shape)\n",
    "adv_l2 = np.asarray(adv_l2_list).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T03:52:24.696483Z",
     "start_time": "2020-04-10T03:52:24.684810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMEAAAAwCAAAAACN50H8AAAEbklEQVR4nO2ZW0wcVRjH/2dmd3Zh1y245Y5cyq3bsi0F0ijaiFhr0MamQWviJSaNFlNvD/quMTH6YGJTNdFUjdoYY9rESJWmraRaWGhroVAuWihrERCE5bqU6ezOzPFhN2qMZ+tshgMafvOwmZndM99///N9Z74zRMdSQc0djtB/3gRzL7N0sASQ/4wCloBVD/ix6sHys+rB8hOXB+rVVm4B3hCmBxb2b5SXRvIrbPxijE0cHmgHvrQ8LS3dQ4dB4siDcMOv8zaqcQwyJsY9oGE7BDmq8G8Pafpf9nVOLhnPAzKxFiniHwrHlJBbuomCREaLfPKE6QFh/IXq2OjFZwrvtzyw1Qbg/LkzQdFhKd9eBkDp+Em9vUSMfO9KM9mymXFNUxUY9kDx7V/E4KEKd3Ea5KaPOkfDgOVk4/7dItB9IFSfmQQAlBz5RHqNocBcDOeBMDgbAl1s7hsE/e7NxqtS7d57HNNdr39OqTXV+cuFcQAACR736wU8BBivRXLBuVdw36F9+dfViwc7w7u+PXzws3c9s33H/Co8HtIxFLn7Bnr1ymIuCpgesO6i5BqkQyl50C4pL3cG854ttyDx4dSnRnqbnyC5m77++dTmdAD4akGojTEpmohhD0jytRaQFJeVdnVNJ72/TSQEQvWr9v5js7BtWKe3DAFA6ISWu4OLgDjmA4swg/4z0LUTa9R9d0gCACLuzNNGeigtKyP9P8rQ0erX70oyuaNnYHxOJmQeer8Oq32BpsmRKIntNoTHgbTKjLmzExRoXBD3cJoZjHsAJRvSrYIAqwtOV/TYjEAXNVDiLaIdAQgTvvDGKk6Tm3EPkLMHMw7oGA+A0EjlocEQrCEAxesTL3Ur6BjQdieqXATE4wHRkKpRDTYZE4Fw5JAUgpQNAvem9FDzFJqV5J3g1CMZ94DSAbglArqOYMZljxxzToPOUAJUZcE3GWhaLPXy6lONe0CDfZgfpWGyIQGtc5FMlhfGiDavgRBvld3f2eindSKf+OPxQBAz4bYDKCpKOH9Y1gAocx/3JHqrrQQQ7s5Rvz+prq0VeT2kxpEHrjpcvvybRJPr1uhvHB2bQrjL9wXxVuQDoLSwQGw8fa0mnVP8cfXJyqdkcrIlqyzx8SNTsy/WF5apb/uGsrbWUBAQZFb+ECCue+06r8UOpgfQGWgNtU7p5oJdVxb1kcecBM7CDIjJT17SNF3XdVX7ptQibOmJ7DFGMBWdtbF7NM+O+oZ2GlBtyHzLcdYfmrSUe20veKJn4S0Z0CrT+a2YMT1g30UFz/kqHpUvOAC43xn+cDhHXr9NyoieJDQjO0GodvBrNpl5wOoyGWjR6kkx+nzTxg88sRSszHc4f5b/0/3y9hRzY4xJHLWIja4Svb1hNO/OJLPDjEEcecBGbmu/3t22sLeU23QGkz2gbe8FlXDVQ26ei0amemB3O2dzKx/x8GmQo5hViyJM9Q6n5t9yo2VtTrUoLgX/Dk5rdv/vdzgrilUPlp9VD1YADAH4HX0LY8I2MU5MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=193x48 at 0x7F48F820CD68>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target_txt[0])\n",
    "show(adv_img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adv_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_const = 50  # float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T16:32:01.649372Z",
     "start_time": "2020-03-31T16:32:01.641408Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim, compare_mse, compare_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T16:32:30.046633Z",
     "start_time": "2020-03-31T16:32:29.888800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.9368019663174"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_mse(adv_img*255, input_img* 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T16:34:09.155864Z",
     "start_time": "2020-03-31T16:34:09.147321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111.466"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(adv_iter_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T16:32:51.518060Z",
     "start_time": "2020-03-31T16:32:51.440175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.93680196631742"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(((adv_img - input_img)*255)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:46:53.041459Z",
     "start_time": "2020-03-24T11:46:53.025953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv_text: fandom target_txt fandom\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=193x48 at 0x7F49C8325D68>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 8\n",
    "print(\"adv_text:\", adv_txt_list[idx],\"target_txt\", target_txt[idx])\n",
    "show(adv_img[idx]) # basic optimized img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wm optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:35:31.117918Z",
     "start_time": "2020-03-24T11:35:30.194573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDNN LSTM backend on GPU\n"
     ]
    }
   ],
   "source": [
    "# build graph\n",
    "with graph.as_default():\n",
    "    # these are variables to be more efficient in sending data to tf\n",
    "    timg = tf.Variable(np.zeros(shape), dtype=tf_dtype, name='timg')\n",
    "    input_seq_len = tf.Variable(np.zeros(batch_size), dtype=tf.int32, name='seq_len')\n",
    "    wm_mask = tf.Variable(np.zeros(shape), dtype=tf_dtype, name='wm_mask')\n",
    "    const = tf.Variable(np.zeros(batch_size), dtype=tf_dtype, name='const')\n",
    "    # and here's what we use to assign them\n",
    "    assign_timg, assign_input_seq_len, targets, dropout_rate, _, _ = network.create_placeholders()\n",
    "    assign_wm_mask = tf.placeholder(tf_dtype, shape, name='assign_const')\n",
    "    assign_const = tf.placeholder(tf_dtype, [batch_size], name='assign_const')\n",
    "    \n",
    "    # the variable we're going to optimize over\n",
    "    modifier = tf.Variable(np.zeros(shape, dtype=np_dtype))\n",
    "    # the resulting instance, tanh'd to keep bounded from clip_min to clip_max\n",
    "    newimg = (tf.tanh(wm_mask * modifier + timg) + 1) / 2\n",
    "    newimg = newimg * (clip_max - clip_min) + clip_min \n",
    "    # prediction BEFORE-SOFTMAX of the model\n",
    "    output_seq_len, time_major_logits, time_major_softmax, logits, softmax, decoded, sparse_decoded, scale_factor, \\\n",
    "            log_prob = network.create_network(newimg, input_seq_len, dropout_rate, reuse_variables=tf.AUTO_REUSE)\n",
    "       \n",
    "    # CTC loss\n",
    "    loss1 = tf.nn.ctc_loss(labels=targets,\n",
    "                           inputs=time_major_logits,\n",
    "                           sequence_length=output_seq_len,\n",
    "                           time_major=True,\n",
    "                           ctc_merge_repeated=True,\n",
    "                           ignore_longer_outputs_than_inputs=True)\n",
    "    loss1 = tf.reduce_sum(const * loss1) # mu * ctc_loss\n",
    "    \n",
    "    # L2-norm distance to the input data\n",
    "    other = (tf.tanh(timg) + 1) / 2 * (clip_max - clip_min) + clip_min  \n",
    "    l2dist = tf.reduce_sum(tf.square(newimg - other), axis=list(range(1, len(shape))))  # (batch_size, )\n",
    "    loss2 = tf.reduce_sum(l2dist) \n",
    "    \n",
    "    # nps: non-printability score\n",
    "    closest_val = tf.divide(tf.round(newimg * 255), 255)  # 乘 255 取整后再除回去\n",
    "    loss3 = tf.reduce_sum(10*tf.abs(newimg - closest_val))\n",
    "    \n",
    "    # loss function\n",
    "    loss = loss1 + loss2\n",
    "    \n",
    "    # Setup the adam optimizer and keep track of variables we're creating\n",
    "    start_vars = set(x.name for x in tf.global_variables())\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "    train = optimizer.minimize(loss, var_list=[modifier])\n",
    "    end_vars = tf.global_variables()\n",
    "    new_vars = [x for x in end_vars if x.name not in start_vars] # 优化在 optimizer 和 train 里的变量\n",
    "    \n",
    "    # these are the variables to initialize when we run\n",
    "    setup = []\n",
    "    setup.append(timg.assign(assign_timg))\n",
    "    setup.append(input_seq_len.assign(assign_input_seq_len))\n",
    "    setup.append(wm_mask.assign(assign_wm_mask))\n",
    "    setup.append(const.assign(assign_const))\n",
    "    \n",
    "    init = tf.variables_initializer(var_list=[modifier] + new_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:35:35.705335Z",
     "start_time": "2020-03-24T11:35:35.633032Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess img\n",
    "imgs = wm0_img[:batch_size]\n",
    "imgs = (imgs - clip_min) / (clip_max - clip_min) # re-scale instances to be within range [0, 1]\n",
    "imgs = np.clip(imgs, clip_min, clip_max)\n",
    "imgs = (imgs * 2) - 1 # now convert to [-1, 1]\n",
    "imgs = np.arctanh(imgs * .999999) # convert to tanh-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:36:20.851187Z",
     "start_time": "2020-03-24T11:35:38.460853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Binary search step 0 of 1\n",
      "Iteration 0 of 1000 loss=26347.5 ctc_loss=26347.5 l2s=0.000 nps=2.89\n",
      "Iteration 100 of 1000 loss=8.9 ctc_loss=8.9 l2s=14.520 nps=5551.92\n",
      "Iteration 200 of 1000 loss=3.4 ctc_loss=3.4 l2s=14.786 nps=5552.57\n",
      "Iteration 300 of 1000 loss=2.1 ctc_loss=2.1 l2s=14.972 nps=5547.55\n",
      "Iteration 400 of 1000 loss=1.5 ctc_loss=1.5 l2s=15.122 nps=5548.18\n",
      "Iteration 500 of 1000 loss=1.2 ctc_loss=1.2 l2s=15.254 nps=5553.76\n",
      "Iteration 600 of 1000 loss=0.9 ctc_loss=0.9 l2s=15.375 nps=5547.26\n",
      "Iteration 700 of 1000 loss=0.8 ctc_loss=0.8 l2s=15.487 nps=5547.44\n",
      "Iteration 800 of 1000 loss=0.6 ctc_loss=0.6 l2s=15.591 nps=5549.95\n",
      "Iteration 900 of 1000 loss=0.5 ctc_loss=0.5 l2s=15.689 nps=5553.52\n",
      "Successfully generated adversarial examples on 100 of 100 instances.\n",
      "Mean successful distortion: 2.486\n",
      "------------------------------ 0 ------------------------------\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    adv_img_list = []\n",
    "    adv_l2_list = []\n",
    "    adv_txt_list = []\n",
    "    for i in range(0, len(imgs), batch_size):  # run attack in batch data\n",
    "        # set the lower and upper bounds accordingly\n",
    "        lower_bound = np.zeros(batch_size)  # (batch_size, )\n",
    "        CONST = np.ones(batch_size) * initial_const  # (batch_size, )\n",
    "        upper_bound = np.ones(batch_size) * 1e10  # (batch_size, )\n",
    "\n",
    "        # placeholders for the best l2, score, and instance attack found so far\n",
    "        o_bestl2 = [1e10] * batch_size  # (batch_size, )\n",
    "        o_bestscore = [-1] * batch_size  # (batch_size, )\n",
    "        o_bestattack = np.zeros(shape)\n",
    "\n",
    "        for outer_step in range(BINARY_SEARCH_STEPS):  # 二分调整 const\n",
    "            # completely reset adam's internal state.\n",
    "            sess.run(init)\n",
    "            batch = imgs[i:i + batch_size]\n",
    "            batch_len_x = len_x[i:i + batch_size]\n",
    "            batch_wm_mask = wm_mask_img[i:i + batch_size]\n",
    "            batch_target_txt = target_txt[i:i + batch_size]\n",
    "            batch_tmp_y = [\n",
    "                np.asarray([c - 1 for c in encode(t)])\n",
    "                for t in batch_target_txt\n",
    "            ]\n",
    "            batch_y = sparse_tuple_from(batch_tmp_y)\n",
    "\n",
    "            bestl2 = [1e10] * batch_size  # (batch_size, )\n",
    "            bestscore = [-1] * batch_size  # (batch_size, )\n",
    "            print(\n",
    "                f\"  Binary search step {outer_step} of {BINARY_SEARCH_STEPS}\")\n",
    "\n",
    "            # set the variables so that we don't have to send them over again\n",
    "            sess.run(\n",
    "                setup, {\n",
    "                    assign_timg: batch,\n",
    "                    assign_input_seq_len: batch_len_x,\n",
    "                    assign_wm_mask: batch_wm_mask,\n",
    "                    assign_const: CONST,\n",
    "                })\n",
    "\n",
    "            for iteration in range(MAX_ITERATIONS):  # 开始迭代攻击\n",
    "                # perform the attack\n",
    "                _, l, scores, l2s, nps, batch_adv_txt, nimg = sess.run(\n",
    "                    [train, loss, loss1, l2dist, loss3, decoded, newimg],\n",
    "                    feed_dict={\n",
    "                        targets: batch_y,\n",
    "                        dropout_rate: 0\n",
    "                    })\n",
    "                batch_adv_index = TensorflowModel._TensorflowModel__sparse_to_lists(batch_adv_txt)\n",
    "                batch_adv_txt = [''.join(decode(index)) for index in batch_adv_index]\n",
    "                # attack done\n",
    "                if iteration % ((MAX_ITERATIONS // 10) or 1) == 0:\n",
    "                    print(f\"Iteration {iteration} of {MAX_ITERATIONS} \"+\n",
    "                          f\"loss={l:.1f} ctc_loss={np.mean(scores):.1f} l2s={np.mean(l2s):.3f}\"+\n",
    "                          f\" nps={nps:.2f}\"\n",
    "                         )\n",
    "                    \n",
    "                \n",
    "                # adjust the best result found so far\n",
    "                for e, (l2, ii) in enumerate(zip(l2s, nimg)):\n",
    "                    target_t, adv_t = batch_target_txt[e], batch_adv_txt[e]\n",
    "                    if l2 < bestl2[e] and target_t == adv_t:\n",
    "                        bestl2[e] = l2\n",
    "                        bestscore[e] = adv_t\n",
    "                    if l2 < o_bestl2[e] and target_t == adv_t:\n",
    "                        o_bestl2[e] = l2\n",
    "                        o_bestscore[e] = adv_t # batch_adv_txt\n",
    "                        o_bestattack[e] = ii\n",
    "                \n",
    "                # check if we should abort search if we're getting nowhere.\n",
    "                if ABORT_EARLY:\n",
    "                    n_success = 0\n",
    "                    for target_t, adv_t in zip(batch_target_txt, bestscore):\n",
    "                        if target_t == adv_t:\n",
    "                            n_success += 1\n",
    "                    if n_success == len(batch_target_txt):\n",
    "                        print(f\"iteration[{iteration}] break, all attacks succeed!\")\n",
    "                        break\n",
    "                        \n",
    "            # adjust the constant as needed\n",
    "            for e in range(batch_size):  # 二分调整 const\n",
    "                if bestscore[e] == batch_target_txt[e]:  # and bestscore[e] != -1:\n",
    "                    # success, divide const by two\n",
    "                    upper_bound[e] = min(upper_bound[e], CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e]) / 2\n",
    "                else:\n",
    "                    # failure, either multiply by 10 if no solution found yet\n",
    "                    # or do binary search with the known upper bound\n",
    "                    lower_bound[e] = max(lower_bound[e], CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e]) / 2\n",
    "                    else:\n",
    "                        CONST[e] *= 10\n",
    "\n",
    "            print(\n",
    "                f\"Successfully generated adversarial examples on {sum(upper_bound < 1e9)} of {batch_size} instances.\"\n",
    "            )\n",
    "            o_bestl2 = np.array(o_bestl2)\n",
    "            mean = np.mean(np.sqrt(o_bestl2[o_bestl2 < 1e9]))\n",
    "            print(f\"Mean successful distortion: {mean:.4g}\")\n",
    "\n",
    "        # return the best solution found\n",
    "        o_bestl2 = np.array(o_bestl2)\n",
    "        adv_l2_list.append(o_bestl2)\n",
    "        adv_txt_list += o_bestscore\n",
    "        adv_img_list.append(o_bestattack) # 把 batch adv img 加到 list 中\n",
    "        print('-' * 30, i, '-' * 30)\n",
    "adv_img = np.asarray(adv_img_list).reshape(imgs.shape)\n",
    "adv_l2 = np.asarray(adv_l2_list).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:36:22.955613Z",
     "start_time": "2020-03-24T11:36:22.941825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status status\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=193x48 at 0x7F4A1C11B710>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 5\n",
    "print(adv_txt_list[idx],target_txt[idx])\n",
    "show(adv_img[idx]) # optimized img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saliency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adv_img[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:51:42.814880Z",
     "start_time": "2020-03-24T11:51:42.806674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7419102145176307"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(imga - imgb).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:52:22.457208Z",
     "start_time": "2020-03-24T11:52:22.332331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f497c2a6ac8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "idx = 66\n",
    "imga = adv_img[idx]\n",
    "imgb = input_img[idx]\n",
    "# imgb = wm0_img[idx]\n",
    "\n",
    "# x = (imga - imgb).T\n",
    "# x = (imga - imgb).T * ((imga - imgb) > 0).T\n",
    "x = (imga - imgb).T * ((imga - imgb) < 0).T\n",
    "\n",
    "plt.axis('off')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(\n",
    "    x,\n",
    "    cmap='RdBu_r',\n",
    "    interpolation='bilinear',\n",
    "    vmin=-0.2,\n",
    "    vmax=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate full image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T09:58:58.921520Z",
     "start_time": "2020-03-24T09:58:58.881419Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_line = 16\n",
    "full_adv_img = np.zeros((48*nb_line, adv_img[0].shape[0]))\n",
    "for i in range(nb_line):\n",
    "    full_adv_img[i*48:(i+1)*48] = 1 - adv_img[i+61].T\n",
    "cvt2Image(full_adv_img).save('opt-real-images/full-4.jpg', quality=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-20T09:41:27.592382Z",
     "start_time": "2020-03-20T09:41:27.446984Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in range(10):\n",
    "    show(adv_img[idx]).save(f'opt-real-images/{target_txt[idx]}.jpg', quality=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# weaken & strengthen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## weaken data\n",
    "basic mim attack: replace-full-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T16:53:42.272279Z",
     "start_time": "2020-02-25T16:53:41.865087Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_data_path = '/home/chenlu/research/TextRecognitionDataGenerator/word_image_data'\n",
    "with open(f'{img_data_path}/Arial-replace-full-word.pkl', 'rb') as f:\n",
    "    img_list, input_img, len_x, gt_txt, target_txt = pickle.load(f)\n",
    "with open(f'attack_result/Arial-replace-full-word-l2-eps0.2-ieps5.0-iter2000.pkl', 'rb') as f:\n",
    "    adv_img, record_adv_text, record_iter, (duration, total_iter) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T17:07:19.260579Z",
     "start_time": "2020-02-25T17:07:19.253491Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_img = adv_img\n",
    "target_txt = gt_txt\n",
    "\n",
    "nb_sample = 100\n",
    "len_x, target_txt = len_x[:nb_sample], target_txt[:nb_sample]\n",
    "input_img = input_img[:nb_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T17:07:20.794615Z",
     "start_time": "2020-02-25T17:07:20.781244Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = 6\n",
    "print(record_adv_text[idx])\n",
    "show(adv_img[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "basic opt attack: replace-full-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T03:32:17.888933Z",
     "start_time": "2020-02-26T03:32:17.770829Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "font_name = 'Arial'\n",
    "case = 'replace-full-word'\n",
    "with open(f'basic_opt_result/{font_name}-{case}.pkl', 'rb') as f:\n",
    "    (input_img, adv_txt_list, adv_l2, adv_iter_list, adv_asr_list, duration) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T03:32:58.245816Z",
     "start_time": "2020-02-26T03:32:58.001021Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(f'{img_data_path}/{font_name}-{case}.pkl', 'rb') as f:\n",
    "    _, _, len_x, gt_txt, target_txt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T03:35:02.689975Z",
     "start_time": "2020-02-26T03:35:02.681991Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "input_img, len_x, target_txt = input_img[:batch_size], len_x[:batch_size], gt_txt[:batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## strengthen attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### gradient attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T17:02:37.599172Z",
     "start_time": "2020-02-25T17:02:37.107281Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# build graph\n",
    "with graph.as_default():\n",
    "    inputs, input_seq_len, targets, dropout_rate, _, _ = network.create_placeholders()\n",
    "    output_seq_len, time_major_logits, time_major_softmax, logits, softmax, decoded, sparse_decoded, scale_factor, log_prob = \\\n",
    "                        network.create_network(inputs, input_seq_len, dropout_rate, reuse_variables=tf.AUTO_REUSE)\n",
    "    loss = tf.nn.ctc_loss(labels=targets,\n",
    "                          inputs=time_major_logits,\n",
    "                          sequence_length=output_seq_len,\n",
    "                          time_major=True,\n",
    "                          ctc_merge_repeated=True,\n",
    "                          ignore_longer_outputs_than_inputs=True)\n",
    "    loss = -tf.reduce_mean(loss, name='loss')\n",
    "    grad, = tf.gradients(loss, inputs)\n",
    "\n",
    "    # Normalize current gradient and add it to the accumulated gradient\n",
    "    red_ind = list(range(1, len(grad.get_shape())))\n",
    "    avoid_zero_div = tf.cast(1e-12, grad.dtype)\n",
    "    divisor = tf.reduce_mean(tf.abs(grad), red_ind, keepdims=True)\n",
    "    norm_grad = grad / tf.maximum(avoid_zero_div, divisor)\n",
    "\n",
    "    m = tf.placeholder(tf.float32,\n",
    "                       shape=inputs.get_shape().as_list(),\n",
    "                       name=\"momentum\")\n",
    "    acc_m = m + norm_grad\n",
    "\n",
    "    grad = acc_m\n",
    "    # ord=np.inf\n",
    "    optimal_perturbation = tf.sign(grad)\n",
    "    optimal_perturbation = tf.stop_gradient(optimal_perturbation)\n",
    "    scaled_perturbation_inf = utils_tf.mul(0.01, optimal_perturbation)\n",
    "    # ord=2\n",
    "    square = tf.maximum(1e-12, tf.reduce_sum(tf.square(grad), axis=red_ind, keepdims=True))\n",
    "    optimal_perturbation = grad / tf.sqrt(square)\n",
    "    scaled_perturbation_2 = utils_tf.mul(0.01, optimal_perturbation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T17:08:03.660538Z",
     "start_time": "2020-02-25T17:07:37.565371Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run attack\n",
    "batch_size = 100\n",
    "clip_min, clip_max = 0.0, 1.0\n",
    "target_index_list = [np.asarray([c for c in encode(t)]) for t in target_txt]\n",
    "with graph.as_default():\n",
    "    adv_img = input_img.copy()\n",
    "    m0 = np.zeros(input_img.shape)\n",
    "    record_iter = np.zeros(input_img.shape[0])  # 0代表没成功\n",
    "\n",
    "    start = time.time()\n",
    "    for i in tqdm(range(nb_iter)):\n",
    "        # perform attack\n",
    "        batch_iter = len(input_img) // batch_size\n",
    "        batch_iter = batch_iter if len(input_img) % batch_size == 0 else batch_iter + 1\n",
    "        for batch_i in range(batch_iter):\n",
    "            batch_input_img = input_img[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_adv_img = adv_img[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_len_x = len_x[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_m0 = m0[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_target_txt = target_txt[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_tmp_y = [np.asarray([c - 1 for c in encode(t)]) for t in batch_target_txt]\n",
    "            batch_y = sparse_tuple_from(batch_tmp_y)\n",
    "            batch_record_iter = record_iter[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "\n",
    "            scaled_perturbation = scaled_perturbation_2 if pert_type == '2' else scaled_perturbation_inf\n",
    "            batch_pert, batch_loss = sess.run([scaled_perturbation, loss], # pert type\n",
    "                                  feed_dict={\n",
    "                                      inputs: batch_adv_img,\n",
    "                                      input_seq_len: batch_len_x,\n",
    "                                      m: batch_m0,\n",
    "                                      targets: batch_y,\n",
    "                                      dropout_rate: 0,\n",
    "                                  })\n",
    "            batch_pert[batch_record_iter != 0] = 0\n",
    "            batch_adv_img = batch_adv_img + eps_iter * batch_pert\n",
    "            batch_adv_img = batch_input_img + np.clip(batch_adv_img - batch_input_img, -eps, eps)\n",
    "            batch_adv_img = np.clip(batch_adv_img, clip_min, clip_max)\n",
    "            adv_img[batch_size * batch_i:batch_size * (batch_i + 1)] = batch_adv_img\n",
    "\n",
    "        # check whether attack success\n",
    "        record_adv_text = []\n",
    "        for batch_i in range(batch_iter):\n",
    "            batch_adv_img = adv_img[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_len_x = len_x[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_target_index = target_index_list[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_adv_text = sess.run(decoded,\n",
    "                                      feed_dict={\n",
    "                                          inputs: batch_adv_img,\n",
    "                                          input_seq_len: batch_len_x,\n",
    "                                          dropout_rate: 0,\n",
    "                                      })\n",
    "            batch_adv_index = TensorflowModel._TensorflowModel__sparse_to_lists(batch_adv_text)\n",
    "            record_adv_text += [''.join(decode(index)) for index in batch_adv_index]\n",
    "            for j in range(len(batch_target_index)):\n",
    "                # attack img idx_j successfully at iter i\n",
    "                idx_j = batch_size * batch_i + j\n",
    "                adv_index, target_index = batch_adv_index[j], batch_target_index[j]\n",
    "                if np.sum(adv_index != target_index) == 0 and record_iter[idx_j] == 0:\n",
    "                    record_iter[idx_j] = i\n",
    "        # check whether all examples are successful\n",
    "        if np.sum(record_iter == 0) == 0:\n",
    "            print(f\"{i} break.\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ocr1]",
   "language": "python",
   "name": "conda-env-ocr1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "422px",
    "width": "249px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
