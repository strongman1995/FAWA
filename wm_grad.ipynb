{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:23:30.674800Z",
     "start_time": "2020-07-09T12:23:30.669204Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, time, io\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:23:31.249412Z",
     "start_time": "2020-07-09T12:23:31.232226Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "def get_argparse():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_path\",\n",
    "                        help=\"Calamari-OCR model path.\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--font_name\",\n",
    "                        help=\"font name.\",\n",
    "                        type=str,\n",
    "                        choices=['Courier',\n",
    "                                 'Georgia',\n",
    "                                 'Helvetica',\n",
    "                                 'Times',\n",
    "                                 'Arial'])\n",
    "    parser.add_argument(\"--case\",\n",
    "                        help=\"case with different targets.\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--pert_type\",\n",
    "                        help=\"the bound type of perturbations\",\n",
    "                        type=str,\n",
    "                        choices=['2', 'inf'])\n",
    "    parser.add_argument(\"--eps\",\n",
    "                        help=\"perturbations is clipped by eps\",\n",
    "                        type=float)\n",
    "    parser.add_argument(\"--eps_iter\",\n",
    "                        help=\"coefficient to adjust step size of each iteration\",\n",
    "                        type=float)\n",
    "    parser.add_argument(\"--nb_iter\",\n",
    "                        help=\"number of maximum iteration\",\n",
    "                        type=int)\n",
    "    parser.add_argument(\"--batch_size\",\n",
    "                        help=\"the number of samples per batch\",\n",
    "                        type=int)\n",
    "    parser.add_argument(\"--clip_min\",\n",
    "                        help=\"the minimum value of images\",\n",
    "                        type=float)\n",
    "    parser.add_argument(\"--clip_max\",\n",
    "                        help=\"the maximum value of images\",\n",
    "                        type=float)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:23:47.520589Z",
     "start_time": "2020-07-09T12:23:32.504114Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle, glob, time, sys, os\n",
    "from tqdm import tqdm\n",
    "from cleverhans import utils_tf\n",
    "from util import cvt2Image, sparse_tuple_from\n",
    "from calamari_ocr.ocr.backends.tensorflow_backend.tensorflow_model import TensorflowModel\n",
    "from calamari_ocr.ocr import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:24:42.899914Z",
     "start_time": "2020-07-09T12:24:42.886703Z"
    }
   },
   "outputs": [],
   "source": [
    "# parse the parameters from shell\n",
    "parser = get_argparse()\n",
    "args = parser.parse_args(\n",
    "    '--model_path 4.ckpt.json \\\n",
    "    --font_name Arial\\\n",
    "    --case easy\\\n",
    "    --pert_type 2\\\n",
    "    --eps 0.2\\\n",
    "    --eps_iter 5\\\n",
    "    --nb_iter 1000\\\n",
    "    --batch_size 100\\\n",
    "    --clip_min 0.0\\\n",
    "    --clip_max 1.0'\n",
    "    .split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:25:31.981691Z",
     "start_time": "2020-07-09T12:25:26.689666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint version 1 is up-to-date.\n",
      "charset: ['', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}']\n",
      "Using CUDNN LSTM backend on GPU\n",
      "Successfully load the model's weights\n",
      "INFO:tensorflow:Restoring parameters from /home/chenlu/research/Fast-Adversarial-Watermark-Attack-on-OCR/ocr_model/4.ckpt\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(checkpoint=os.path.join(\"ocr_model\", args.model_path), batch_size=1, processes=10)\n",
    "network = predictor.network\n",
    "sess, graph = network.session, network.graph\n",
    "encode, decode = network.codec.encode, network.codec.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:25:39.416576Z",
     "start_time": "2020-07-09T12:25:38.889895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDNN LSTM backend on GPU\n"
     ]
    }
   ],
   "source": [
    "# build graph\n",
    "with graph.as_default():\n",
    "    inputs, input_seq_len, targets, dropout_rate, _, _ = network.create_placeholders()\n",
    "    output_seq_len, time_major_logits, time_major_softmax, logits, softmax, decoded, sparse_decoded, scale_factor, log_prob = \\\n",
    "        network.create_network(inputs, input_seq_len, dropout_rate, reuse_variables=tf.AUTO_REUSE)\n",
    "    loss = tf.nn.ctc_loss(labels=targets,\n",
    "                          inputs=time_major_logits,\n",
    "                          sequence_length=output_seq_len,\n",
    "                          time_major=True,\n",
    "                          ctc_merge_repeated=True,\n",
    "                          ignore_longer_outputs_than_inputs=True)\n",
    "    loss = -tf.reduce_mean(loss, name='loss')\n",
    "    grad, = tf.gradients(loss, inputs)\n",
    "\n",
    "    # Normalize current gradient and add it to the accumulated gradient\n",
    "    red_ind = list(range(1, len(grad.get_shape())))\n",
    "    avoid_zero_div = tf.cast(1e-12, grad.dtype)\n",
    "    divisor = tf.reduce_mean(tf.abs(grad), red_ind, keepdims=True)\n",
    "    norm_grad = grad / tf.maximum(avoid_zero_div, divisor)\n",
    "\n",
    "    m = tf.placeholder(tf.float32,\n",
    "                       shape=inputs.get_shape().as_list(),\n",
    "                       name=\"momentum\")\n",
    "    acc_m = m + norm_grad\n",
    "\n",
    "    # watermark mask\n",
    "    mask = tf.placeholder(tf.float32,\n",
    "                          shape=inputs.get_shape().as_list(),\n",
    "                          name=\"mask\")\n",
    "    grad = tf.multiply(acc_m, mask, name=\"mask_op\")\n",
    "    \n",
    "    # ord=np.inf\n",
    "    optimal_perturbation = tf.sign(grad)\n",
    "    optimal_perturbation = tf.stop_gradient(optimal_perturbation)\n",
    "    scaled_perturbation_inf = utils_tf.mul(0.01, optimal_perturbation)\n",
    "    # ord=1\n",
    "    abs_grad = tf.abs(grad)\n",
    "    max_abs_grad = tf.reduce_max(abs_grad, axis=red_ind, keepdims=True)\n",
    "    tied_for_max = tf.to_float(tf.equal(abs_grad, max_abs_grad))\n",
    "    num_ties = tf.reduce_sum(tied_for_max, axis=red_ind, keepdims=True)\n",
    "    optimal_perturbation = tf.sign(grad) * tied_for_max / num_ties\n",
    "    scaled_perturbation_1 = utils_tf.mul(0.01, optimal_perturbation)\n",
    "    # ord=2\n",
    "    square = tf.maximum(1e-12, tf.reduce_sum(tf.square(grad), axis=red_ind, keepdims=True))\n",
    "    optimal_perturbation = grad / tf.sqrt(square)\n",
    "    scaled_perturbation_2 = utils_tf.mul(0.01, optimal_perturbation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:25:50.678066Z",
     "start_time": "2020-07-09T12:25:50.669546Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "font_name = args.font_name\n",
    "case = args.case\n",
    "pert_type = args.pert_type\n",
    "eps = args.eps\n",
    "eps_iter = args.eps_iter\n",
    "nb_iter = args.nb_iter\n",
    "batch_size = args.batch_size\n",
    "clip_min, clip_max = args.clip_min, args.clip_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:26:06.350384Z",
     "start_time": "2020-07-09T12:26:05.834513Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load img data\n",
    "with open(f'img_data/{font_name}.pkl', 'rb') as f:\n",
    "    input_img, len_x, gt_txt = pickle.load(f)\n",
    "# load attack pair\n",
    "with open(f'attack_pair/{font_name}-{case}.pkl', 'rb') as f:\n",
    "    _, target_txt = pickle.load(f)\n",
    "# load basic_grad result\n",
    "with open(f'attack_result/{title}.pkl', 'rb') as f:\n",
    "    adv_img, record_adv_text, record_iter, (duration, total_iter) = pickle.load(f)\n",
    "# adv_img, record_adv_text, record_iter = adv_img[:100], record_adv_text[:100], record_iter[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# small samples\n",
    "n_img = 200\n",
    "input_img, len_x, gt_txt, target_txt = input_img[:n_img], len_x[:n_img], gt_txt[:n_img], target_txt[:n_img]\n",
    "adv_img, record_adv_text, record_iter = adv_img, record_adv_text, record_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load image data\n",
    "with open(f'{img_data_path}/{font_name}-{case}.pkl', 'rb') as f:\n",
    "    img_list, input_img, len_x, gt_txt, target_txt = pickle.load(f)\n",
    "input_img = np.asarray(input_img)\n",
    "# input_img, len_x, gt_txt, target_txt = input_img[:100], len_x[:100], gt_txt[:100], target_txt[:100]\n",
    "\n",
    "title = f\"{font_name}-{case}-l{pert_type}-eps{eps}-ieps{eps_iter}-iter{nb_iter}\"\n",
    "# title = f\"{font_name}-{case}-l{pert_type}-eps{eps}-ieps5.0-iter{nb_iter}\"\n",
    "with open(f'attack_result/{title}.pkl', 'rb') as f:\n",
    "# with open(f'sample_images_calamari/grad-basic-{title}.pkl', 'rb') as f:\n",
    "    adv_img, record_adv_text, record_iter, (duration, total_iter) = pickle.load(f)\n",
    "# adv_img, record_adv_text, record_iter = adv_img[:100], record_adv_text[:100], record_iter[:100]\n",
    "\n",
    "\n",
    "from skimage import morphology\n",
    "import cv2\n",
    "\n",
    "\n",
    "def find_wm_pos(adv_img, input_img, ret_frame_img=False):\n",
    "    pert = np.abs(cvt2raw(adv_img) - cvt2raw(input_img))\n",
    "    pert = (pert > 1e-2) * 255.0\n",
    "    wm_pos_list = []\n",
    "    frame_img_list = []\n",
    "    for src in pert:\n",
    "        kernel = np.ones((3, 3), np.uint8)  # 设置卷积核3*3\n",
    "        dilate = cv2.dilate(src, kernel, iterations=2)  # 图像的膨胀\n",
    "        erode = cv2.erode(dilate, kernel, iterations=2)  # 图像的腐蚀\n",
    "        remove = morphology.remove_small_objects(erode.astype('bool'), min_size=0)\n",
    "        contours, _ = cv2.findContours((remove * 255).astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        wm_pos, frame_img = [], []\n",
    "        for cont in contours:\n",
    "            left_point = cont.min(axis=1).min(axis=0)\n",
    "            right_point = cont.max(axis=1).max(axis=0)\n",
    "            wm_pos.append(np.hstack((left_point, right_point)))\n",
    "            if ret_frame_img:\n",
    "                img = cv2.rectangle(\n",
    "                    (remove * 255).astype('uint8'), (left_point[0], left_point[1]),\n",
    "                    (right_point[0], right_point[1]), (255, 255, 255), 2)\n",
    "                frame_img.append(img)\n",
    "        wm_pos_list.append(wm_pos)\n",
    "        frame_img_list.append(frame_img)\n",
    "\n",
    "    if ret_frame_img:\n",
    "        return (wm_pos_list, frame_img_list)\n",
    "    else:\n",
    "        return wm_pos_list\n",
    "\n",
    "pos, frames = find_wm_pos(adv_img, input_img, True)\n",
    "\n",
    "# 按面积大小把pos从大到小排个序\n",
    "new_pos = []\n",
    "for _pos in pos:\n",
    "    if len(_pos) > 1:\n",
    "        new_pos.append(sorted(_pos, key=lambda x: (x[3]-x[1])*(x[2]-x[0]), reverse=True))\n",
    "    else:\n",
    "        new_pos.append(_pos)\n",
    "pos = new_pos\n",
    "\n",
    "from trdg.generators import GeneratorFromStrings\n",
    "\n",
    "def gen_wm(RGB):\n",
    "    generator = GeneratorFromStrings(\n",
    "        strings=['eccv'],\n",
    "        count=1,  # 五种字体\n",
    "        fonts=['Impact.ttf'],  # default: []\n",
    "        language='en',\n",
    "        size=100,  # 32\n",
    "        skewing_angle=10,\n",
    "        random_skew=False,\n",
    "        blur=0,\n",
    "        random_blur=False,\n",
    "        # gaussian noise (0), plain white (1), quasicrystal (2) or picture (3)\n",
    "        background_type=1,\n",
    "        distorsion_type=0,  # None(0), Sine wave(1),Cosine wave(2),Random(3)\n",
    "        distorsion_orientation=0,\n",
    "        is_handwritten=False,\n",
    "        width=-1,\n",
    "        alignment=1,\n",
    "        text_color=RGB2Hex(RGB),\n",
    "        orientation=0,\n",
    "        space_width=1.0,\n",
    "        character_spacing=0,\n",
    "        margins=(0, 0, 0, 0),\n",
    "        fit=True,\n",
    "    )\n",
    "    img_list = [img for img, _ in generator]\n",
    "    return img_list[0]\n",
    "\n",
    "\n",
    "# RGB格式颜色转换为16进制颜色格式\n",
    "def RGB2Hex(RGB): # RGB is a 3-tuple\n",
    "    color = '#'\n",
    "    for num in RGB:\n",
    "        color += str(hex(num))[-2:].replace('x', '0').upper()\n",
    "    return color\n",
    "\n",
    "# 得到水印 mask\n",
    "grayscale = 0 # 灰度值在 76-226 之间有对应的彩色水印值\n",
    "color = (grayscale, grayscale, grayscale)\n",
    "wm_img = gen_wm(color)\n",
    "wm_arr = np.array(wm_img.convert('L'))\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "wm_arr = cv2.dilate(wm_arr, kernel, 2)\n",
    "wm_arr = cv2.erode(wm_arr, kernel, 2)\n",
    "bg_mask = ~(wm_arr != 255)\n",
    "\n",
    "# 灰色水印\n",
    "# grayscale = int(sys.argv[7])\n",
    "grayscale = 174 # 灰度值在 76-226 有对应的彩色水印值，为了增加扰动后还在范围内，128-174\n",
    "color = (grayscale, grayscale, grayscale)\n",
    "wm_img = np.array(Image.new(mode=\"RGB\", size=wm_img.size, color=color))\n",
    "wm_img[bg_mask] = 255\n",
    "wm_img = Image.fromarray(wm_img)\n",
    "\n",
    "# large_l = []\n",
    "# green_val = np.array(list(range(256)))\n",
    "# for _gi in range(256):\n",
    "#     _r, _g, _b = 255, _gi, 0\n",
    "#     large_l.append(_r * 19595 + _g * 38470 + _b * 7471 + 0x8000)\n",
    "# normal_l = np.array(large_l, dtype='uint32') >> 16\n",
    "# l0_1 = normal_l / 255\n",
    "# range_min, range_max = l0_1.min(), l0_1.max()\n",
    "# gray_green_map = dict(list(zip(normal_l, green_val)))\n",
    "# gray_green_map_array = np.ones((255, ))\n",
    "# gray_green_map_array[:76] = gray_green_map[76]\n",
    "# gray_green_map_array[227:] = gray_green_map[226]\n",
    "# for gray, green in gray_green_map.items():\n",
    "#     gray_green_map_array[gray] = green\n",
    "\n",
    "# 彩色水印\n",
    "# green_v = gray_green_map[grayscale]\n",
    "# color = (255, green_v, 0)\n",
    "# wm_img = np.array(Image.new(mode=\"RGB\", size=wm_img.size, color=color))\n",
    "# wm_img[bg_mask] = 255\n",
    "# wm_img = Image.fromarray(wm_img)\n",
    "\n",
    "def get_text_mask(img: np.array):\n",
    "    if img.max() <= 1:\n",
    "        return img < 1 / 1.25\n",
    "    else:\n",
    "        return img < 255 / 1.25\n",
    "\n",
    "\n",
    "wm0_img_list = []\n",
    "wm_mask_list = []\n",
    "text_mask_list = []\n",
    "for i in range(len(input_img)):\n",
    "    text_img = show(input_img[i])\n",
    "    text_mask = get_text_mask(np.array(text_img))  # 得到 text 的 mask (bool)\n",
    "    rgb_img = Image.new(mode=\"RGB\", size=text_img.size, color=(255, 255, 255))\n",
    "    p = -int(wm_img.size[0] * np.tan(10 * np.pi / 180))\n",
    "    right_shift = 10\n",
    "    xp = pos[i][0][0]+right_shift if len(pos[i]) != 0 else right_shift\n",
    "    # xp = 0\n",
    "    rgb_img.paste(wm_img, box=(xp, p))  # 先贴 wm\n",
    "    wm_mask = (np.array(rgb_img.convert('L')) != 255)  # 得到 wm 的 mask(bool)\n",
    "    rgb_img.paste(text_img, mask=cvt2Image(text_mask))  # 再贴 text\n",
    "\n",
    "    wm0_img_list.append(rgb_img)\n",
    "    wm_mask_list.append(transpose(wm_mask))\n",
    "    text_mask_list.append(transpose(text_mask))\n",
    "wm_mask = np.asarray(wm_mask_list)\n",
    "text_mask = np.asarray(text_mask_list)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "clip_min, clip_max = 0.0, 1.0\n",
    "\n",
    "# 大数据集查看\n",
    "record_text = []\n",
    "wm0_img = pred_img = np.asarray([cvt2raw(np.array(img.convert('L'))) / 255 for img in wm0_img_list])\n",
    "batch_iter = len(input_img) // batch_size\n",
    "batch_iter = batch_iter if len(input_img) % batch_size == 0 else batch_iter + 1\n",
    "for batch_i in range(batch_iter):\n",
    "    batch_img = pred_img[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "    batch_len_x = len_x[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "    batch_text = sess.run(decoded,\n",
    "                              feed_dict={\n",
    "                                  inputs: batch_img,\n",
    "                                  input_seq_len: batch_len_x,\n",
    "                                  dropout_rate: 0,\n",
    "                              })\n",
    "    batch_index = TensorflowModel._TensorflowModel__sparse_to_lists(batch_text)\n",
    "    record_text += [''.join(decode(index)) for index in batch_index]\n",
    "\n",
    "cnt = 0\n",
    "for pred_txt, raw_txt in zip(record_text, gt_txt):\n",
    "    if pred_txt == raw_txt:\n",
    "        cnt += 1\n",
    "\n",
    "accuracy = cnt / len(gt_txt)\n",
    "\n",
    "# run attack\n",
    "\n",
    "target_index_list = [np.asarray([c for c in encode(t)]) for t in target_txt]\n",
    "wm_img = wm0_img\n",
    "with graph.as_default():\n",
    "    adv_img = wm_img.copy()\n",
    "    m0 = np.zeros(input_img.shape)\n",
    "    record_iter = np.zeros(input_img.shape[0])  # 0代表没成功\n",
    "    record_mse = []\n",
    "    record_mse_plus = []\n",
    "    start = time.time()\n",
    "    for i in tqdm(range(nb_iter)):\n",
    "        batch_iter = len(input_img) // batch_size\n",
    "        batch_iter = batch_iter if len(input_img) % batch_size == 0 else batch_iter + 1\n",
    "        for batch_i in range(batch_iter):\n",
    "            batch_input_img = wm_img[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_adv_img = adv_img[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_len_x = len_x[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_m0 = m0[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_target_txt = target_txt[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_tmp_y = [np.asarray([c - 1 for c in encode(t)]) for t in batch_target_txt]\n",
    "            batch_y = sparse_tuple_from(batch_tmp_y)\n",
    "            batch_mask = wm_mask[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_record_iter = record_iter[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "\n",
    "            scaled_perturbation = scaled_perturbation_2 if pert_type == '2' else scaled_perturbation_inf\n",
    "            batch_pert = sess.run(scaled_perturbation,\n",
    "                                  feed_dict={\n",
    "                                      inputs: batch_adv_img,\n",
    "                                      input_seq_len: batch_len_x,\n",
    "                                      m: batch_m0,\n",
    "                                      targets: batch_y,\n",
    "                                      mask: batch_mask,\n",
    "                                      dropout_rate: 0,\n",
    "                                  })\n",
    "            batch_pert[batch_record_iter != 0] = 0\n",
    "            batch_adv_img = batch_adv_img + eps_iter * batch_pert  * (batch_pert > 0) # negative\n",
    "            batch_adv_img = batch_input_img + np.clip(batch_adv_img - batch_input_img, -eps, eps)\n",
    "            batch_adv_img = np.clip(batch_adv_img, clip_min, clip_max)\n",
    "            adv_img[batch_size * batch_i:batch_size * (batch_i + 1)] = batch_adv_img\n",
    "        record_mse.append(np.mean(((adv_img - wm_img) * 255) ** 2))\n",
    "        record_mse_plus.append(np.mean((((adv_img - wm_img) * ((adv_img - wm_img) > 0)) * 255) ** 2))\n",
    "\n",
    "        record_adv_text = []\n",
    "        for batch_i in range(batch_iter):\n",
    "            batch_adv_img = adv_img[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_len_x = len_x[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_target_index = target_index_list[batch_size * batch_i:batch_size * (batch_i + 1)]\n",
    "            batch_adv_text = sess.run(decoded,\n",
    "                                      feed_dict={\n",
    "                                          inputs: batch_adv_img,\n",
    "                                          input_seq_len: batch_len_x,\n",
    "                                          dropout_rate: 0,\n",
    "                                      })\n",
    "            batch_adv_index = TensorflowModel._TensorflowModel__sparse_to_lists(batch_adv_text)\n",
    "            record_adv_text += [''.join(decode(index)) for index in batch_adv_index]\n",
    "            for j in (range(len(batch_target_index))):\n",
    "                # attack img j successfully at iter i\n",
    "                adv_index, target_index = batch_adv_index[j], batch_target_index[j]\n",
    "                idx_j = batch_size * batch_i + j\n",
    "                if np.sum(adv_index != target_index) == 0 and record_iter[idx_j] == 0:\n",
    "                    record_iter[idx_j] = i\n",
    "        if np.sum(record_iter == 0) == 0:  # all examples are successful\n",
    "            break\n",
    "    duration = time.time() - start\n",
    "    print(f\"{i} break. Time cost {duration:.4f} s\")\n",
    "\n",
    "def cvt2rgb(gray_img, text_mask):\n",
    "    gray_img = invert(gray_img)\n",
    "    op_mask = (~(gray_img == 1)) & (~text_mask) # not_bg & not_text\n",
    "    rgb_img = np.ones(list(gray_img.shape)+[3])\n",
    "    rgb_img[:, :, :, 0] = gray_img\n",
    "    rgb_img[:, :, :, 1] = gray_img\n",
    "    rgb_img[:, :, :, 2] = gray_img\n",
    "    rgb_img[op_mask, 0] = 1\n",
    "    rgb_img[op_mask, 1] = (gray_img[op_mask] - 0.299) / 0.587\n",
    "    rgb_img[op_mask, 2] = 0\n",
    "    return invert(rgb_img)\n",
    "\n",
    "rgb_img = cvt2rgb(adv_img, text_mask)\n",
    "\n",
    "\n",
    "# with open(f'wm_result/{font_name}-{case}-mse.pkl', 'wb') as f:\n",
    "#     pickle.dump((record_mse, record_mse_plus), f)\n",
    "\n",
    "# title = f\"{font_name}-{case}-l{pert_type}-eps{eps}-ieps{eps_iter}-iter{nb_iter}\"\n",
    "# title = f\"{font_name}-{case}-l{pert_type}-eps{eps}-ieps{eps_iter}-iter{nb_iter}-gray{int(sys.argv[7])}\"\n",
    "title = f\"{font_name}-{case}-l{pert_type}-eps{eps}-ieps{eps_iter}-iter{nb_iter}-positive\"\n",
    "with open(f'wm_result/{title}.pkl', 'wb') as f:\n",
    "# # with open(f'sample_images_calamari/grad-wm-{title}.pkl', 'wb') as f:\n",
    "    pickle.dump((pos, wm_mask, text_mask, wm0_img, record_text, accuracy, adv_img, record_adv_text, record_iter, (duration, i), rgb_img), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T12:15:36.521126Z",
     "start_time": "2020-07-06T12:15:36.514243Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_data_path = \"/home/chenlu/research/TextRecognitionDataGenerator/word_image_data\"\n",
    "fonts = ['Courier', 'Georgia', 'Helvetica', 'times', 'Arial']\n",
    "cases = ['easy', 'random', 'hard', 'insert', 'delete', 'replace-full-word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T12:19:05.583519Z",
     "start_time": "2020-07-06T12:15:59.117576Z"
    }
   },
   "outputs": [],
   "source": [
    "for font_name in fonts:\n",
    "    for case in cases:\n",
    "        # load data\n",
    "        with open(f'{img_data_path}/{font_name}-{case}.pkl', 'rb') as f:\n",
    "            img_list, input_img, len_x, gt_txt, target_txt = pickle.load(f)\n",
    "        input_img = np.asarray(input_img)\n",
    "        print(f'{font_name} {case} {len(input_img)}')\n",
    "        # save img data\n",
    "        with open(f'img_data/{font_name.title()}-{case}.pkl', 'wb') as f:\n",
    "            pickle.dump((input_img, len_x, gt_txt), f)\n",
    "        # save attack pair\n",
    "        with open(f'attack_pair/{font_name.title()}-{case}.pkl', 'wb') as f:\n",
    "            pickle.dump((gt_txt, target_txt), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ocr1]",
   "language": "python",
   "name": "conda-env-ocr1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
